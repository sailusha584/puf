{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHYSICAL UNCLONABLE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "      <th>1.41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  -1  1.2  -1.1  -1.2  -1.3  1.3  1.4  -1.4  ...   1.33  1.34  1.35  \\\n",
       "0 -1    1   1    1     1     1     1    1   -1    -1  ...     -1     1    -1   \n",
       "1 -1   -1   1    1    -1     1     1    1    1    -1  ...      1    -1     1   \n",
       "2  1   -1  -1    1    -1     1     1    1   -1     1  ...      1    -1     1   \n",
       "3  1    1   1   -1    -1    -1    -1    1    1    -1  ...     -1     1    -1   \n",
       "4 -1    1  -1   -1     1    -1    -1   -1   -1    -1  ...     -1     1    -1   \n",
       "\n",
       "   1.36  1.37  1.38  1.39  1.40  -1.22  1.41  \n",
       "0     1    -1     1     1    -1      1    -1  \n",
       "1    -1     1    -1    -1    -1      1     1  \n",
       "2    -1     1    -1    -1     1     -1    -1  \n",
       "3     1    -1     1    -1    -1      1     1  \n",
       "4     1    -1    -1     1     1      1     1  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puf=pd.read_csv(\"./XOR_Arbiter_PUFs/XOR_Arbiter_PUFs/6xor_64bit/train.csv\")\n",
    "puf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "puf=puf.replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading data into csv file\n",
    " puf.to_csv('./tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puf=pd.read_csv(\"./tr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=puf[puf.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          0\n",
       "3          1\n",
       "4          1\n",
       "5          0\n",
       "6          0\n",
       "7          0\n",
       "8          1\n",
       "9          0\n",
       "10         0\n",
       "11         1\n",
       "12         1\n",
       "13         1\n",
       "14         1\n",
       "15         0\n",
       "16         0\n",
       "17         0\n",
       "18         0\n",
       "19         0\n",
       "20         1\n",
       "21         1\n",
       "22         0\n",
       "23         0\n",
       "24         0\n",
       "25         0\n",
       "26         0\n",
       "27         1\n",
       "28         0\n",
       "29         0\n",
       "          ..\n",
       "1999969    1\n",
       "1999970    0\n",
       "1999971    0\n",
       "1999972    1\n",
       "1999973    0\n",
       "1999974    0\n",
       "1999975    0\n",
       "1999976    0\n",
       "1999977    0\n",
       "1999978    1\n",
       "1999979    1\n",
       "1999980    1\n",
       "1999981    1\n",
       "1999982    0\n",
       "1999983    0\n",
       "1999984    1\n",
       "1999985    0\n",
       "1999986    0\n",
       "1999987    0\n",
       "1999988    1\n",
       "1999989    0\n",
       "1999990    0\n",
       "1999991    0\n",
       "1999992    0\n",
       "1999993    1\n",
       "1999994    0\n",
       "1999995    0\n",
       "1999996    1\n",
       "1999997    1\n",
       "1999998    0\n",
       "Name: 1.41, Length: 1999999, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999999,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "puf=puf.drop(columns='1.41')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.32</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  -1  1.2  -1.1  -1.2  -1.3  1.3  1.4  -1.4  ...    1.32  1.33  1.34  \\\n",
       "0  0    1   1    1     1     1     1    1    0     0  ...       1     0     1   \n",
       "1  0    0   1    1     0     1     1    1    1     0  ...       0     1     0   \n",
       "2  1    0   0    1     0     1     1    1    0     1  ...       0     1     0   \n",
       "3  1    1   1    0     0     0     0    1    1     0  ...       1     0     1   \n",
       "4  0    1   0    0     1     0     0    0    0     0  ...       1     0     1   \n",
       "\n",
       "   1.35  1.36  1.37  1.38  1.39  1.40  -1.22  \n",
       "0     0     1     0     1     1     0      1  \n",
       "1     1     0     1     0     0     0      1  \n",
       "2     1     0     1     0     0     1      0  \n",
       "3     0     1     0     1     0     0      1  \n",
       "4     0     1     0     0     1     1      1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1000019\n",
       "0     999980\n",
       "Name: 1.41, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking if the dataset is imbalanced\n",
    "res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999999 entries, 0 to 1999998\n",
      "Data columns (total 65 columns):\n",
      "1        int64\n",
      "1.1      int64\n",
      "-1       int64\n",
      "1.2      int64\n",
      "-1.1     int64\n",
      "-1.2     int64\n",
      "-1.3     int64\n",
      "1.3      int64\n",
      "1.4      int64\n",
      "-1.4     int64\n",
      "1.5      int64\n",
      "-1.5     int64\n",
      "1.6      int64\n",
      "1.7      int64\n",
      "-1.6     int64\n",
      "1.8      int64\n",
      "1.9      int64\n",
      "1.10     int64\n",
      "1.11     int64\n",
      "1.12     int64\n",
      "-1.7     int64\n",
      "1.13     int64\n",
      "-1.8     int64\n",
      "-1.9     int64\n",
      "1.14     int64\n",
      "1.15     int64\n",
      "1.16     int64\n",
      "-1.10    int64\n",
      "1.17     int64\n",
      "1.18     int64\n",
      "1.19     int64\n",
      "1.20     int64\n",
      "-1.11    int64\n",
      "-1.12    int64\n",
      "-1.13    int64\n",
      "-1.14    int64\n",
      "-1.15    int64\n",
      "-1.16    int64\n",
      "-1.17    int64\n",
      "-1.18    int64\n",
      "-1.19    int64\n",
      "-1.20    int64\n",
      "1.21     int64\n",
      "1.22     int64\n",
      "-1.21    int64\n",
      "1.23     int64\n",
      "1.24     int64\n",
      "1.25     int64\n",
      "1.26     int64\n",
      "1.27     int64\n",
      "1.28     int64\n",
      "1.29     int64\n",
      "1.30     int64\n",
      "1.31     int64\n",
      "1.32     int64\n",
      "1.33     int64\n",
      "1.34     int64\n",
      "1.35     int64\n",
      "1.36     int64\n",
      "1.37     int64\n",
      "1.38     int64\n",
      "1.39     int64\n",
      "1.40     int64\n",
      "-1.22    int64\n",
      "1.41     int64\n",
      "dtypes: int64(65)\n",
      "memory usage: 991.8 MB\n"
     ]
    }
   ],
   "source": [
    "puf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "      <th>1.41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "      <td>1.999999e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.157501e-03</td>\n",
       "      <td>9.115005e-04</td>\n",
       "      <td>4.275002e-04</td>\n",
       "      <td>-1.585001e-04</td>\n",
       "      <td>-1.755001e-04</td>\n",
       "      <td>-3.015002e-04</td>\n",
       "      <td>8.405004e-04</td>\n",
       "      <td>6.485003e-04</td>\n",
       "      <td>-5.695003e-04</td>\n",
       "      <td>3.615002e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.845003e-04</td>\n",
       "      <td>2.075001e-04</td>\n",
       "      <td>-9.665005e-04</td>\n",
       "      <td>-1.450001e-05</td>\n",
       "      <td>-3.375002e-04</td>\n",
       "      <td>1.312501e-03</td>\n",
       "      <td>-2.575001e-04</td>\n",
       "      <td>-6.500003e-06</td>\n",
       "      <td>-1.365001e-04</td>\n",
       "      <td>1.950001e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1           1.1            -1           1.2          -1.1  \\\n",
       "count  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06   \n",
       "mean   1.157501e-03  9.115005e-04  4.275002e-04 -1.585001e-04 -1.755001e-04   \n",
       "std    9.999996e-01  9.999998e-01  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "50%    1.000000e+00  1.000000e+00  1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "75%    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "               -1.2          -1.3           1.3           1.4          -1.4  \\\n",
       "count  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06   \n",
       "mean  -3.015002e-04  8.405004e-04  6.485003e-04 -5.695003e-04  3.615002e-04   \n",
       "std    1.000000e+00  9.999999e-01  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "50%   -1.000000e+00  1.000000e+00  1.000000e+00 -1.000000e+00  1.000000e+00   \n",
       "75%    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "           ...               1.33          1.34          1.35          1.36  \\\n",
       "count      ...       1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06   \n",
       "mean       ...       5.845003e-04  2.075001e-04 -9.665005e-04 -1.450001e-05   \n",
       "std        ...       1.000000e+00  1.000000e+00  9.999998e-01  1.000000e+00   \n",
       "min        ...      -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%        ...      -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "50%        ...       1.000000e+00  1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "75%        ...       1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max        ...       1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "               1.37          1.38          1.39          1.40         -1.22  \\\n",
       "count  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06  1.999999e+06   \n",
       "mean  -3.375002e-04  1.312501e-03 -2.575001e-04 -6.500003e-06 -1.365001e-04   \n",
       "std    1.000000e+00  9.999994e-01  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%   -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "50%   -1.000000e+00  1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "75%    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "               1.41  \n",
       "count  1.999999e+06  \n",
       "mean   1.950001e-05  \n",
       "std    1.000000e+00  \n",
       "min   -1.000000e+00  \n",
       "25%   -1.000000e+00  \n",
       "50%    1.000000e+00  \n",
       "75%    1.000000e+00  \n",
       "max    1.000000e+00  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pufa=puf[puf.duplicated(keep='last')]\n",
    "print(pufa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=puf.eq(puf.iloc[:, 0], axis=0).all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=puf[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "      <th>1.41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  -1  1.2  -1.1  -1.2  -1.3  1.3  1.4  -1.4  ...   1.33  1.34  1.35  \\\n",
       "0  0    1   1    1     1     1     1    1    0     0  ...      0     1     0   \n",
       "\n",
       "   1.36  1.37  1.38  1.39  1.40  -1.22  1.41  \n",
       "0     1     0     1     1     0      1     0  \n",
       "\n",
       "[1 rows x 65 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b['-1']=b.loc[:,'-1'].replace(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=b.drop(columns='1.41')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puf[0:1]=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>-1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.32</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  -1  1.2  -1.1  -1.2  -1.3  1.3  1.4  -1.4  ...    1.32  1.33  1.34  \\\n",
       "0  0    0   1    1     1     1     1    1    0     0  ...       1     0     1   \n",
       "1  0    0   1    1     0     1     1    1    1     0  ...       0     1     0   \n",
       "2  1    0   0    1     0     1     1    1    0     1  ...       0     1     0   \n",
       "3  1    1   1    0     0     0     0    1    1     0  ...       1     0     1   \n",
       "4  0    1   0    0     1     0     0    0    0     0  ...       1     0     1   \n",
       "\n",
       "   1.35  1.36  1.37  1.38  1.39  1.40  -1.22  \n",
       "0     0     1     0     1     1     0      1  \n",
       "1     1     0     1     0     0     0      1  \n",
       "2     1     0     1     0     0     1      0  \n",
       "3     0     1     0     1     0     0      1  \n",
       "4     0     1     0     0     1     1      1  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1=puf.eq(puf.iloc[:, 0], axis=0).all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.tolist().count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>1.5</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.32</th>\n",
       "      <th>-1.33</th>\n",
       "      <th>-1.34</th>\n",
       "      <th>-1.35</th>\n",
       "      <th>-1.36</th>\n",
       "      <th>-1.37</th>\n",
       "      <th>-1.38</th>\n",
       "      <th>-1.39</th>\n",
       "      <th>1.23</th>\n",
       "      <th>-1.40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1  -1.1  1  1.1  -1.2  1.2  1.3  1.4  1.5  -1.3  ...    -1.32  -1.33  \\\n",
       "0   1     1 -1    1    -1   -1    1    1   -1     1  ...       -1     -1   \n",
       "1   1     1  1   -1     1    1   -1    1   -1     1  ...       -1     -1   \n",
       "2   1    -1  1   -1    -1   -1   -1   -1   -1     1  ...        1      1   \n",
       "3   1    -1  1   -1     1   -1   -1    1    1     1  ...        1      1   \n",
       "4   1    -1 -1    1    -1   -1   -1   -1    1    -1  ...        1     -1   \n",
       "\n",
       "   -1.34  -1.35  -1.36  -1.37  -1.38  -1.39  1.23  -1.40  \n",
       "0     -1     -1     -1     -1      1      1    -1      1  \n",
       "1     -1     -1     -1     -1      1      1     1      1  \n",
       "2      1      1      1      1      1      1     1      1  \n",
       "3      1      1      1      1      1      1     1     -1  \n",
       "4      1     -1      1     -1     -1      1    -1     -1  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv(\"./XOR_Arbiter_PUFs/XOR_Arbiter_PUFs/6xor_64bit/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv(\"./tes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"./tes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>1.5</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>...</th>\n",
       "      <th>-1.32</th>\n",
       "      <th>-1.33</th>\n",
       "      <th>-1.34</th>\n",
       "      <th>-1.35</th>\n",
       "      <th>-1.36</th>\n",
       "      <th>-1.37</th>\n",
       "      <th>-1.38</th>\n",
       "      <th>-1.39</th>\n",
       "      <th>1.23</th>\n",
       "      <th>-1.40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1  -1.1  1  1.1  -1.2  1.2  1.3  1.4  1.5  -1.3  ...    -1.32  -1.33  \\\n",
       "0   1     1  0    1     0    0    1    1    0     1  ...        0      0   \n",
       "1   1     1  1    0     1    1    0    1    0     1  ...        0      0   \n",
       "2   1     0  1    0     0    0    0    0    0     1  ...        1      1   \n",
       "3   1     0  1    0     1    0    0    1    1     1  ...        1      1   \n",
       "4   1     0  0    1     0    0    0    0    1     0  ...        1      0   \n",
       "\n",
       "   -1.34  -1.35  -1.36  -1.37  -1.38  -1.39  1.23  -1.40  \n",
       "0      0      0      0      0      1      1     0      1  \n",
       "1      0      0      0      0      1      1     1      1  \n",
       "2      1      1      1      1      1      1     1      1  \n",
       "3      1      1      1      1      1      1     1      0  \n",
       "4      1      0      1      0      0      1     0      0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1=test[test.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop(columns='-1.40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [-1, -1.1, 1, 1.1, -1.2, 1.2, 1.3, 1.4, 1.5, -1.3, 1.6, 1.7, -1.4, -1.5, -1.6, 1.8, -1.7, 1.9, -1.8, 1.10, -1.9, 1.11, 1.12, -1.10, 1.13, 1.14, 1.15, -1.11, 1.16, -1.12, -1.13, -1.14, 1.17, 1.18, 1.19, -1.15, 1.20, -1.16, -1.17, -1.18, -1.19, 1.21, -1.20, 1.22, -1.21, -1.22, -1.23, -1.24, -1.25, -1.26, -1.27, -1.28, -1.29, -1.30, -1.31, -1.32, -1.33, -1.34, -1.35, -1.36, -1.37, -1.38, -1.39, 1.23, -1.40]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "test=test[test.duplicated(keep=False)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=test.eq(test.iloc[:, 0], axis=0).all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist().count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL-1: LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples sepal:  1399999\n",
      "# testing samples sepal:  600000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.learning_curve import validation_curve\n",
    "### 1. Use of validation curves for both datasets.\n",
    "#C_param_range = [0.001,0.01,0.1,1,10,100,1000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(puf,res,test_size=0.3,random_state=0)\n",
    "\n",
    "print(\"# training samples sepal: \", len(X_train))\n",
    "print(\"# testing samples sepal: \", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression(C=0.01,penalty='l1')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Entire Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression(C=0.01,penalty='l1')\n",
    "logreg.fit(puf,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.50\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(test, res1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.50\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf=LogisticRegressionCV(cv=5, random_state=0, multi_class='ovr').fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_parameter  Accuracy\n",
      "0        0.001  0.499435\n",
      "1        0.010  0.499367\n",
      "2        0.100  0.499052\n",
      "3        1.000  0.499325\n",
      "4       10.000  0.499285\n",
      "5      100.000  0.499295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2030ef33320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "puf_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n",
    "puf_acc_table['C_parameter'] = C_param_range\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    lr = LogisticRegression(penalty = 'l1', C = i,random_state = 0)\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    puf_acc_table.iloc[j,1] = accuracy_score(y_test,y_pred)\n",
    "    j += 1\n",
    "print(puf_acc_table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>-1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>-1.1</th>\n",
       "      <th>-1.2</th>\n",
       "      <th>-1.3</th>\n",
       "      <th>1.3</th>\n",
       "      <th>1.4</th>\n",
       "      <th>...</th>\n",
       "      <th>1.32</th>\n",
       "      <th>1.33</th>\n",
       "      <th>1.34</th>\n",
       "      <th>1.35</th>\n",
       "      <th>1.36</th>\n",
       "      <th>1.37</th>\n",
       "      <th>1.38</th>\n",
       "      <th>1.39</th>\n",
       "      <th>1.40</th>\n",
       "      <th>-1.22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>641457</th>\n",
       "      <td>641457</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452992</th>\n",
       "      <td>452992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928498</th>\n",
       "      <td>928498</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722388</th>\n",
       "      <td>722388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903300</th>\n",
       "      <td>1903300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  1  1.1  -1  1.2  -1.1  -1.2  -1.3  1.3  1.4  ...    1.32  \\\n",
       "641457       641457  0    1   1    1     0     0     1    1    1  ...       0   \n",
       "452992       452992  0    0   1    1     1     1     1    0    1  ...       1   \n",
       "928498       928498  1    0   1    0     1     0     1    0    0  ...       0   \n",
       "722388       722388  0    0   1    1     1     0     1    0    0  ...       0   \n",
       "1903300     1903300  1    0   1    0     1     1     0    0    1  ...       0   \n",
       "\n",
       "         1.33  1.34  1.35  1.36  1.37  1.38  1.39  1.40  -1.22  \n",
       "641457      1     0     1     0     0     1     1     1      1  \n",
       "452992      0     1     0     1     1     0     0     1      0  \n",
       "928498      1     1     1     1     1     1     0     0      1  \n",
       "722388      1     1     0     1     1     1     1     1      1  \n",
       "1903300     0     0     0     1     0     0     0     0      0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    700319\n",
      "1    699680\n",
      "Name: 1.41, dtype: int64\n",
      "1    300339\n",
      "0    299661\n",
      "Name: 1.41, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_parameter  Accuracy\n",
      "0        0.001  0.499565\n",
      "1        0.010  0.499563\n",
      "2        0.100  0.499563\n",
      "3        1.000  0.499563\n",
      "4       10.000  0.499563\n",
      "5      100.000  0.499563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2030eaa5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "puf_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n",
    "puf_acc_table['C_parameter'] = C_param_range\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
    "    lr.fit(trainp,y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_pred = lr.predict(testp)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    puf_acc_table.iloc[j,1] = accuracy_score(y_test,y_pred)\n",
    "    j += 1\n",
    "print(puf_acc_table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL-2 : XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0x2030071c668>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_c=xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 0.001, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_class.fit(X_train,y_train)\n",
    "\n",
    "preds = xgb_class.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499606666667\n"
     ]
    }
   ],
   "source": [
    "acc=accuracy_score(y_test,preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.505491\tvalidation_0-error:0.496046\tvalidation_1-auc:0.500197\tvalidation_1-error:0.499982\n",
      "[1]\tvalidation_0-auc:0.506175\tvalidation_0-error:0.495686\tvalidation_1-auc:0.500221\tvalidation_1-error:0.500037\n",
      "[2]\tvalidation_0-auc:0.508545\tvalidation_0-error:0.494062\tvalidation_1-auc:0.500164\tvalidation_1-error:0.500463\n",
      "[3]\tvalidation_0-auc:0.510327\tvalidation_0-error:0.492633\tvalidation_1-auc:0.500604\tvalidation_1-error:0.499192\n",
      "[4]\tvalidation_0-auc:0.511753\tvalidation_0-error:0.491601\tvalidation_1-auc:0.501555\tvalidation_1-error:0.498927\n",
      "[5]\tvalidation_0-auc:0.512605\tvalidation_0-error:0.491145\tvalidation_1-auc:0.501538\tvalidation_1-error:0.498528\n",
      "[6]\tvalidation_0-auc:0.513366\tvalidation_0-error:0.490422\tvalidation_1-auc:0.501182\tvalidation_1-error:0.499082\n",
      "[7]\tvalidation_0-auc:0.514023\tvalidation_0-error:0.490447\tvalidation_1-auc:0.500919\tvalidation_1-error:0.499395\n",
      "[8]\tvalidation_0-auc:0.514713\tvalidation_0-error:0.489488\tvalidation_1-auc:0.50143\tvalidation_1-error:0.49869\n",
      "[9]\tvalidation_0-auc:0.514967\tvalidation_0-error:0.489444\tvalidation_1-auc:0.501583\tvalidation_1-error:0.498537\n",
      "[10]\tvalidation_0-auc:0.515086\tvalidation_0-error:0.489343\tvalidation_1-auc:0.501607\tvalidation_1-error:0.498517\n",
      "[11]\tvalidation_0-auc:0.51586\tvalidation_0-error:0.489487\tvalidation_1-auc:0.501424\tvalidation_1-error:0.499118\n",
      "[12]\tvalidation_0-auc:0.516257\tvalidation_0-error:0.488841\tvalidation_1-auc:0.501226\tvalidation_1-error:0.499598\n",
      "[13]\tvalidation_0-auc:0.516324\tvalidation_0-error:0.488797\tvalidation_1-auc:0.501154\tvalidation_1-error:0.49969\n",
      "[14]\tvalidation_0-auc:0.516364\tvalidation_0-error:0.488827\tvalidation_1-auc:0.501175\tvalidation_1-error:0.49962\n",
      "[15]\tvalidation_0-auc:0.516823\tvalidation_0-error:0.48834\tvalidation_1-auc:0.501485\tvalidation_1-error:0.498972\n",
      "[16]\tvalidation_0-auc:0.517617\tvalidation_0-error:0.48785\tvalidation_1-auc:0.502029\tvalidation_1-error:0.499067\n",
      "[17]\tvalidation_0-auc:0.517918\tvalidation_0-error:0.48754\tvalidation_1-auc:0.501892\tvalidation_1-error:0.499117\n",
      "[18]\tvalidation_0-auc:0.518353\tvalidation_0-error:0.487112\tvalidation_1-auc:0.501643\tvalidation_1-error:0.49924\n",
      "[19]\tvalidation_0-auc:0.518772\tvalidation_0-error:0.486863\tvalidation_1-auc:0.501806\tvalidation_1-error:0.49907\n",
      "[20]\tvalidation_0-auc:0.518991\tvalidation_0-error:0.486715\tvalidation_1-auc:0.501857\tvalidation_1-error:0.498708\n",
      "[21]\tvalidation_0-auc:0.519571\tvalidation_0-error:0.48654\tvalidation_1-auc:0.501896\tvalidation_1-error:0.498715\n",
      "[22]\tvalidation_0-auc:0.519874\tvalidation_0-error:0.486081\tvalidation_1-auc:0.502075\tvalidation_1-error:0.498548\n",
      "[23]\tvalidation_0-auc:0.520134\tvalidation_0-error:0.485877\tvalidation_1-auc:0.501975\tvalidation_1-error:0.498622\n",
      "[24]\tvalidation_0-auc:0.520183\tvalidation_0-error:0.485864\tvalidation_1-auc:0.502033\tvalidation_1-error:0.498593\n",
      "[25]\tvalidation_0-auc:0.520555\tvalidation_0-error:0.485695\tvalidation_1-auc:0.502124\tvalidation_1-error:0.498493\n",
      "[26]\tvalidation_0-auc:0.520859\tvalidation_0-error:0.485409\tvalidation_1-auc:0.502304\tvalidation_1-error:0.4986\n",
      "[27]\tvalidation_0-auc:0.521379\tvalidation_0-error:0.485048\tvalidation_1-auc:0.502334\tvalidation_1-error:0.498803\n",
      "[28]\tvalidation_0-auc:0.52185\tvalidation_0-error:0.484858\tvalidation_1-auc:0.502563\tvalidation_1-error:0.498648\n",
      "[29]\tvalidation_0-auc:0.522267\tvalidation_0-error:0.484254\tvalidation_1-auc:0.502683\tvalidation_1-error:0.498932\n",
      "[30]\tvalidation_0-auc:0.522488\tvalidation_0-error:0.48433\tvalidation_1-auc:0.502651\tvalidation_1-error:0.498697\n",
      "[31]\tvalidation_0-auc:0.522912\tvalidation_0-error:0.484073\tvalidation_1-auc:0.502388\tvalidation_1-error:0.498602\n",
      "[32]\tvalidation_0-auc:0.522951\tvalidation_0-error:0.484192\tvalidation_1-auc:0.502379\tvalidation_1-error:0.498747\n",
      "[33]\tvalidation_0-auc:0.523218\tvalidation_0-error:0.483848\tvalidation_1-auc:0.502413\tvalidation_1-error:0.498502\n",
      "[34]\tvalidation_0-auc:0.523222\tvalidation_0-error:0.48385\tvalidation_1-auc:0.502408\tvalidation_1-error:0.498495\n",
      "[35]\tvalidation_0-auc:0.523585\tvalidation_0-error:0.483831\tvalidation_1-auc:0.502462\tvalidation_1-error:0.498762\n",
      "[36]\tvalidation_0-auc:0.524044\tvalidation_0-error:0.483175\tvalidation_1-auc:0.502465\tvalidation_1-error:0.498735\n",
      "[37]\tvalidation_0-auc:0.524152\tvalidation_0-error:0.483245\tvalidation_1-auc:0.502743\tvalidation_1-error:0.498425\n",
      "[38]\tvalidation_0-auc:0.524207\tvalidation_0-error:0.48321\tvalidation_1-auc:0.502798\tvalidation_1-error:0.498362\n",
      "[39]\tvalidation_0-auc:0.524555\tvalidation_0-error:0.4828\tvalidation_1-auc:0.502832\tvalidation_1-error:0.498335\n",
      "[40]\tvalidation_0-auc:0.524878\tvalidation_0-error:0.482957\tvalidation_1-auc:0.503042\tvalidation_1-error:0.49846\n",
      "[41]\tvalidation_0-auc:0.524996\tvalidation_0-error:0.482822\tvalidation_1-auc:0.503018\tvalidation_1-error:0.49849\n",
      "[42]\tvalidation_0-auc:0.52534\tvalidation_0-error:0.482528\tvalidation_1-auc:0.502964\tvalidation_1-error:0.49858\n",
      "[43]\tvalidation_0-auc:0.525604\tvalidation_0-error:0.482423\tvalidation_1-auc:0.502952\tvalidation_1-error:0.498137\n",
      "[44]\tvalidation_0-auc:0.525889\tvalidation_0-error:0.482347\tvalidation_1-auc:0.503052\tvalidation_1-error:0.498425\n",
      "[45]\tvalidation_0-auc:0.526049\tvalidation_0-error:0.482402\tvalidation_1-auc:0.503345\tvalidation_1-error:0.49821\n",
      "[46]\tvalidation_0-auc:0.526046\tvalidation_0-error:0.482399\tvalidation_1-auc:0.503342\tvalidation_1-error:0.498217\n",
      "[47]\tvalidation_0-auc:0.526358\tvalidation_0-error:0.482097\tvalidation_1-auc:0.50336\tvalidation_1-error:0.498135\n",
      "[48]\tvalidation_0-auc:0.526486\tvalidation_0-error:0.4818\tvalidation_1-auc:0.503369\tvalidation_1-error:0.498007\n",
      "[49]\tvalidation_0-auc:0.526481\tvalidation_0-error:0.481821\tvalidation_1-auc:0.503376\tvalidation_1-error:0.498002\n",
      "[50]\tvalidation_0-auc:0.526661\tvalidation_0-error:0.481596\tvalidation_1-auc:0.503343\tvalidation_1-error:0.497987\n",
      "[51]\tvalidation_0-auc:0.526849\tvalidation_0-error:0.481545\tvalidation_1-auc:0.503374\tvalidation_1-error:0.497725\n",
      "[52]\tvalidation_0-auc:0.526968\tvalidation_0-error:0.481535\tvalidation_1-auc:0.503445\tvalidation_1-error:0.49772\n",
      "[53]\tvalidation_0-auc:0.527263\tvalidation_0-error:0.481245\tvalidation_1-auc:0.503401\tvalidation_1-error:0.497937\n",
      "[54]\tvalidation_0-auc:0.527479\tvalidation_0-error:0.481058\tvalidation_1-auc:0.503384\tvalidation_1-error:0.497973\n",
      "[55]\tvalidation_0-auc:0.527658\tvalidation_0-error:0.480825\tvalidation_1-auc:0.503446\tvalidation_1-error:0.49841\n",
      "[56]\tvalidation_0-auc:0.527936\tvalidation_0-error:0.480487\tvalidation_1-auc:0.503547\tvalidation_1-error:0.498128\n",
      "[57]\tvalidation_0-auc:0.528163\tvalidation_0-error:0.480406\tvalidation_1-auc:0.503561\tvalidation_1-error:0.498487\n",
      "[58]\tvalidation_0-auc:0.528177\tvalidation_0-error:0.480399\tvalidation_1-auc:0.503572\tvalidation_1-error:0.49848\n",
      "[59]\tvalidation_0-auc:0.528301\tvalidation_0-error:0.480444\tvalidation_1-auc:0.503784\tvalidation_1-error:0.497948\n",
      "[60]\tvalidation_0-auc:0.528481\tvalidation_0-error:0.48032\tvalidation_1-auc:0.503804\tvalidation_1-error:0.497773\n",
      "[61]\tvalidation_0-auc:0.528779\tvalidation_0-error:0.480123\tvalidation_1-auc:0.503861\tvalidation_1-error:0.497783\n",
      "[62]\tvalidation_0-auc:0.529073\tvalidation_0-error:0.479731\tvalidation_1-auc:0.503876\tvalidation_1-error:0.497968\n",
      "[63]\tvalidation_0-auc:0.529075\tvalidation_0-error:0.479823\tvalidation_1-auc:0.50392\tvalidation_1-error:0.497755\n",
      "[64]\tvalidation_0-auc:0.529348\tvalidation_0-error:0.479631\tvalidation_1-auc:0.503978\tvalidation_1-error:0.498418\n",
      "[65]\tvalidation_0-auc:0.529341\tvalidation_0-error:0.47963\tvalidation_1-auc:0.503985\tvalidation_1-error:0.498428\n",
      "[66]\tvalidation_0-auc:0.529571\tvalidation_0-error:0.479621\tvalidation_1-auc:0.503964\tvalidation_1-error:0.498187\n",
      "[67]\tvalidation_0-auc:0.529752\tvalidation_0-error:0.4795\tvalidation_1-auc:0.504064\tvalidation_1-error:0.498298\n",
      "[68]\tvalidation_0-auc:0.529857\tvalidation_0-error:0.479435\tvalidation_1-auc:0.504088\tvalidation_1-error:0.498243\n",
      "[69]\tvalidation_0-auc:0.53006\tvalidation_0-error:0.479237\tvalidation_1-auc:0.504077\tvalidation_1-error:0.49826\n",
      "[70]\tvalidation_0-auc:0.530256\tvalidation_0-error:0.479262\tvalidation_1-auc:0.504075\tvalidation_1-error:0.49813\n",
      "[71]\tvalidation_0-auc:0.530264\tvalidation_0-error:0.479254\tvalidation_1-auc:0.504084\tvalidation_1-error:0.498098\n",
      "[72]\tvalidation_0-auc:0.530271\tvalidation_0-error:0.479249\tvalidation_1-auc:0.504078\tvalidation_1-error:0.4981\n",
      "[73]\tvalidation_0-auc:0.530472\tvalidation_0-error:0.478758\tvalidation_1-auc:0.504141\tvalidation_1-error:0.497478\n",
      "[74]\tvalidation_0-auc:0.53065\tvalidation_0-error:0.478663\tvalidation_1-auc:0.504157\tvalidation_1-error:0.497502\n",
      "[75]\tvalidation_0-auc:0.530895\tvalidation_0-error:0.478623\tvalidation_1-auc:0.504183\tvalidation_1-error:0.497238\n",
      "[76]\tvalidation_0-auc:0.531073\tvalidation_0-error:0.478482\tvalidation_1-auc:0.504312\tvalidation_1-error:0.497535\n",
      "[77]\tvalidation_0-auc:0.531195\tvalidation_0-error:0.478364\tvalidation_1-auc:0.504354\tvalidation_1-error:0.49743\n",
      "[78]\tvalidation_0-auc:0.531331\tvalidation_0-error:0.478208\tvalidation_1-auc:0.504297\tvalidation_1-error:0.497338\n",
      "[79]\tvalidation_0-auc:0.531581\tvalidation_0-error:0.477867\tvalidation_1-auc:0.504423\tvalidation_1-error:0.497185\n",
      "[80]\tvalidation_0-auc:0.531803\tvalidation_0-error:0.477963\tvalidation_1-auc:0.504487\tvalidation_1-error:0.496815\n",
      "[81]\tvalidation_0-auc:0.532124\tvalidation_0-error:0.477752\tvalidation_1-auc:0.504467\tvalidation_1-error:0.49675\n",
      "[82]\tvalidation_0-auc:0.532119\tvalidation_0-error:0.477706\tvalidation_1-auc:0.504458\tvalidation_1-error:0.496658\n",
      "[83]\tvalidation_0-auc:0.532201\tvalidation_0-error:0.477505\tvalidation_1-auc:0.504364\tvalidation_1-error:0.496538\n",
      "[84]\tvalidation_0-auc:0.532337\tvalidation_0-error:0.477512\tvalidation_1-auc:0.504217\tvalidation_1-error:0.497107\n",
      "[85]\tvalidation_0-auc:0.5325\tvalidation_0-error:0.477414\tvalidation_1-auc:0.504203\tvalidation_1-error:0.497117\n",
      "[86]\tvalidation_0-auc:0.532728\tvalidation_0-error:0.477246\tvalidation_1-auc:0.504247\tvalidation_1-error:0.497395\n",
      "[87]\tvalidation_0-auc:0.532904\tvalidation_0-error:0.477113\tvalidation_1-auc:0.504266\tvalidation_1-error:0.497438\n",
      "[88]\tvalidation_0-auc:0.533064\tvalidation_0-error:0.47698\tvalidation_1-auc:0.504276\tvalidation_1-error:0.497155\n",
      "[89]\tvalidation_0-auc:0.533168\tvalidation_0-error:0.47676\tvalidation_1-auc:0.504223\tvalidation_1-error:0.49736\n",
      "[90]\tvalidation_0-auc:0.533391\tvalidation_0-error:0.476572\tvalidation_1-auc:0.504274\tvalidation_1-error:0.497312\n",
      "[91]\tvalidation_0-auc:0.533629\tvalidation_0-error:0.476365\tvalidation_1-auc:0.504302\tvalidation_1-error:0.497252\n",
      "[92]\tvalidation_0-auc:0.533803\tvalidation_0-error:0.47624\tvalidation_1-auc:0.504351\tvalidation_1-error:0.4968\n",
      "[93]\tvalidation_0-auc:0.533929\tvalidation_0-error:0.475972\tvalidation_1-auc:0.504379\tvalidation_1-error:0.496988\n",
      "[94]\tvalidation_0-auc:0.534136\tvalidation_0-error:0.475841\tvalidation_1-auc:0.504369\tvalidation_1-error:0.497177\n",
      "[95]\tvalidation_0-auc:0.534302\tvalidation_0-error:0.475808\tvalidation_1-auc:0.504298\tvalidation_1-error:0.49737\n",
      "[96]\tvalidation_0-auc:0.534488\tvalidation_0-error:0.47555\tvalidation_1-auc:0.504328\tvalidation_1-error:0.497537\n",
      "[97]\tvalidation_0-auc:0.534561\tvalidation_0-error:0.4755\tvalidation_1-auc:0.504337\tvalidation_1-error:0.497543\n",
      "[98]\tvalidation_0-auc:0.534785\tvalidation_0-error:0.475522\tvalidation_1-auc:0.504408\tvalidation_1-error:0.497275\n",
      "[99]\tvalidation_0-auc:0.534918\tvalidation_0-error:0.475376\tvalidation_1-auc:0.504315\tvalidation_1-error:0.497208\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.001, base_score=0.5, booster='gbtree',\n",
       "       colsample_bylevel=1, colsample_bytree=0.3, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "eval_metric = [\"auc\",\"error\"]\n",
    "%time xgb_c.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using k-fold cross validation with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(xgb_class1, puf, res, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%.2f [ 0.499625    0.501085    0.50188     0.501615    0.501635    0.50237\n",
      "  0.50226     0.501755    0.499305    0.50130751]\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.13% \n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%% \" % (results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.19% \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(xgb_class1, puf, res, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% \" % (results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   C_parameter Accuracy\n",
      "0        0.001  0.50245\n",
      "1        0.010  0.50245\n",
      "2        0.100  0.50245\n",
      "3        1.000  0.50245\n",
      "4       10.000  0.50245\n",
      "5      100.000  0.50245\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.learning_curve import validation_curve\n",
    "import xgboost as xgb\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "puf_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\n",
    "puf_acc_table['C_parameter'] = C_param_range\n",
    "\n",
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "j = 0\n",
    "for i in C_param_range:\n",
    "    \n",
    "   \n",
    "    xgb_class1=xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = i, n_estimators = 10)\n",
    "    # Predict using model\n",
    "    xgb_class1.fit(X_train,y_train)\n",
    "\n",
    "    predi = xgb_class1.predict(X_test)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    puf_acc_table.iloc[j,1] = accuracy_score(y_test,predi)\n",
    "    j += 1\n",
    "print(puf_acc_table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using kfold cross-validation with logistic regreesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=0.001,penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(logreg, puf, res, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.93% \n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%% \" % (results.mean()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MODEL_3:SVM with linear svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn.datasets import make_classification\n",
    "#X, y = make_classification(n_features=4, random_state=0)\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.500565\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "svmpred=clf.predict(X_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, svmpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MODEL_4 :MLP CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#X =puf\n",
    "#y =res\n",
    "mlpe = MLPClassifier(activation='relu',solver='adam', alpha=1e-5,batch_size=10000,early_stopping=True,learning_rate='adaptive',learning_rate_init=0.001,max_iter=200,\n",
    "                     hidden_layer_sizes=(64,64,64),random_state=1)\n",
    "mlpe.fit(puf,res) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size=10000, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(64, 64, 64), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=20, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=mlpe.predict(test)\n",
    "yt=mlpe.predict(puf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986409966024915\n",
      "0.987840993920497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(res1,y))\n",
    "print(accuracy_score(res,yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=5, random_state=7)\n",
    "results = cross_val_score(karma, puf, res, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.500765   0.5007625  0.9833     0.4998775  0.50085625]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.71% \n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%% \" % (results.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#X =puf\n",
    "#y =res\n",
    "karm= MLPClassifier(activation='relu',solver='adam', alpha=1e-5,batch_size=10000,early_stopping=True,learning_rate='adaptive',learning_rate_init=0.001,max_iter=200,\n",
    "                     hidden_layer_sizes=(64,64,64),random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size=10000, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(64, 64, 64), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "karm.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=karm.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.49983333333333335\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',accuracy_score(y_test1,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1=karm.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5000312500781252\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',accuracy_score(res1,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.71% \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold1 = StratifiedKFold(n_splits=10, random_state=7)\n",
    "results1 = cross_val_score(k, puf, res, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% \" % (results1.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.500765  , 0.5007625 , 0.9833    , 0.4998775 , 0.50085625])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9873399683499209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [0.0001, 0.01],'learning_rate': ['constant','adaptive']}\n",
    "clf = GridSearchCV(MLPClassifier(activation='relu',solver='adam',batch_size=10000,learning_rate_init=0.001,early_stopping=True,\n",
    "                     hidden_layer_sizes=(64,64,64),random_state=1), parameters, n_jobs=-1)\n",
    "\n",
    "clf.fit(puf,res)\n",
    "#print(clf.score(X_test,y_test))\n",
    "y1=clf.predict(test)\n",
    "print('accuracy:',accuracy_score(res1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.501415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'alpha': [0.0001, 0.01],'learning_rate': ['constant','adaptive'],'learning_rate_init':[0.001,0.1]}\n",
    "clf = GridSearchCV(MLPClassifier(activation='relu',solver='adam',batch_size=10000,learning_rate_init=0.001,early_stopping=True,\n",
    "                     hidden_layer_sizes=(64,64,64),random_state=1), parameters, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train1,y_train1)\n",
    "#print(clf.score(X_test,y_test))\n",
    "y1=clf.predict(X_test1)\n",
    "print('accuracy:',accuracy_score(y_test1,y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL_5:Sequential model using keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 1\n",
    "input_dim = puf.shape[1]\n",
    "\n",
    "batch_size = 10000\n",
    "nb_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 16,705\n",
      "Trainable params: 16,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_re = Sequential()\n",
    "model_re.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
    "model_re.add(Dense(64, activation='relu'))\n",
    "model_re.add(Dense(64, activation='relu'))\n",
    "model_re.add(Dense(64, activation='relu'))\n",
    "model_re.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "model_re.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999999 samples, validate on 399999 samples\n",
      "Epoch 1/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.6933 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5009\n",
      "Epoch 2/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.6931 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.5001\n",
      "Epoch 3/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.6931 - acc: 0.5047 - val_loss: 0.6932 - val_acc: 0.5017\n",
      "Epoch 4/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.6930 - acc: 0.5063 - val_loss: 0.6932 - val_acc: 0.5016\n",
      "Epoch 5/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.6928 - acc: 0.5085 - val_loss: 0.6930 - val_acc: 0.5034\n",
      "Epoch 6/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.6918 - acc: 0.5115 - val_loss: 0.6903 - val_acc: 0.5142\n",
      "Epoch 7/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.6648 - acc: 0.5669 - val_loss: 0.5976 - val_acc: 0.6559\n",
      "Epoch 8/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.4897 - acc: 0.7418 - val_loss: 0.4008 - val_acc: 0.8045\n",
      "Epoch 9/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.3363 - acc: 0.8428 - val_loss: 0.2826 - val_acc: 0.8743\n",
      "Epoch 10/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.2409 - acc: 0.8956 - val_loss: 0.2060 - val_acc: 0.9132\n",
      "Epoch 11/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.1825 - acc: 0.9244 - val_loss: 0.1613 - val_acc: 0.9339\n",
      "Epoch 12/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.1470 - acc: 0.9405 - val_loss: 0.1374 - val_acc: 0.9432\n",
      "Epoch 13/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.1238 - acc: 0.9506 - val_loss: 0.1172 - val_acc: 0.9527\n",
      "Epoch 14/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.1064 - acc: 0.9579 - val_loss: 0.1005 - val_acc: 0.9601\n",
      "Epoch 15/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0943 - acc: 0.9628 - val_loss: 0.0884 - val_acc: 0.9656\n",
      "Epoch 16/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0843 - acc: 0.9670 - val_loss: 0.0819 - val_acc: 0.9672\n",
      "Epoch 17/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0762 - acc: 0.9702 - val_loss: 0.0727 - val_acc: 0.9720\n",
      "Epoch 18/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0712 - acc: 0.9718 - val_loss: 0.0655 - val_acc: 0.9749\n",
      "Epoch 19/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0648 - acc: 0.9747 - val_loss: 0.0609 - val_acc: 0.9771\n",
      "Epoch 20/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0601 - acc: 0.9765 - val_loss: 0.0667 - val_acc: 0.9723\n",
      "Epoch 21/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0567 - acc: 0.9776 - val_loss: 0.0579 - val_acc: 0.9768\n",
      "Epoch 22/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0543 - acc: 0.9784 - val_loss: 0.0550 - val_acc: 0.9782\n",
      "Epoch 23/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0504 - acc: 0.9800 - val_loss: 0.0487 - val_acc: 0.9808\n",
      "Epoch 24/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0486 - acc: 0.9806 - val_loss: 0.0547 - val_acc: 0.9778\n",
      "Epoch 25/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0467 - acc: 0.9814 - val_loss: 0.0479 - val_acc: 0.9804\n",
      "Epoch 26/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0437 - acc: 0.9827 - val_loss: 0.0433 - val_acc: 0.9827\n",
      "Epoch 27/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0416 - acc: 0.9836 - val_loss: 0.0438 - val_acc: 0.9820\n",
      "Epoch 28/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0409 - acc: 0.9836 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 29/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0403 - acc: 0.9837 - val_loss: 0.0376 - val_acc: 0.9851\n",
      "Epoch 30/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0382 - acc: 0.9845 - val_loss: 0.0384 - val_acc: 0.9842\n",
      "Epoch 31/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0375 - acc: 0.9848 - val_loss: 0.0404 - val_acc: 0.9832\n",
      "Epoch 32/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0365 - acc: 0.9852 - val_loss: 0.0386 - val_acc: 0.9840\n",
      "Epoch 33/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0345 - acc: 0.9861 - val_loss: 0.0356 - val_acc: 0.9855\n",
      "Epoch 34/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0337 - acc: 0.9864 - val_loss: 0.0385 - val_acc: 0.9839\n",
      "Epoch 35/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0332 - acc: 0.9865 - val_loss: 0.0371 - val_acc: 0.9843\n",
      "Epoch 36/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0333 - acc: 0.9864 - val_loss: 0.0305 - val_acc: 0.9880\n",
      "Epoch 37/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0308 - acc: 0.9875 - val_loss: 0.0280 - val_acc: 0.9891\n",
      "Epoch 38/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0320 - acc: 0.9869 - val_loss: 0.0489 - val_acc: 0.9807\n",
      "Epoch 39/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0302 - acc: 0.9877 - val_loss: 0.0329 - val_acc: 0.9862\n",
      "Epoch 40/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0301 - acc: 0.9876 - val_loss: 0.0298 - val_acc: 0.9877\n",
      "Epoch 41/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0302 - acc: 0.9877 - val_loss: 0.0306 - val_acc: 0.9870\n",
      "Epoch 42/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0317 - acc: 0.9869 - val_loss: 0.0457 - val_acc: 0.9812\n",
      "Epoch 43/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0325 - acc: 0.9867 - val_loss: 0.0317 - val_acc: 0.9871\n",
      "Epoch 44/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0278 - acc: 0.9887 - val_loss: 0.0453 - val_acc: 0.9817\n",
      "Epoch 45/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0287 - acc: 0.9882 - val_loss: 0.0267 - val_acc: 0.9891\n",
      "Epoch 46/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0290 - acc: 0.9880 - val_loss: 0.0379 - val_acc: 0.9842\n",
      "Epoch 47/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0266 - acc: 0.9892 - val_loss: 0.0310 - val_acc: 0.9871\n",
      "Epoch 48/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0288 - acc: 0.9881 - val_loss: 0.0346 - val_acc: 0.9850\n",
      "Epoch 49/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0278 - acc: 0.9885 - val_loss: 0.0262 - val_acc: 0.9890\n",
      "Epoch 50/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0264 - acc: 0.9891 - val_loss: 0.0272 - val_acc: 0.9887\n",
      "Epoch 51/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0278 - acc: 0.9884 - val_loss: 0.0269 - val_acc: 0.9886\n",
      "Epoch 52/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0266 - acc: 0.9889 - val_loss: 0.0261 - val_acc: 0.9890\n",
      "Epoch 53/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0261 - acc: 0.9891 - val_loss: 0.0281 - val_acc: 0.9884\n",
      "Epoch 54/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0267 - acc: 0.9889 - val_loss: 0.0307 - val_acc: 0.9880\n",
      "Epoch 55/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0247 - acc: 0.9898 - val_loss: 0.0256 - val_acc: 0.9893\n",
      "Epoch 56/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0257 - acc: 0.9893 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 57/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0261 - acc: 0.9891 - val_loss: 0.0285 - val_acc: 0.9876\n",
      "Epoch 58/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0237 - acc: 0.9902 - val_loss: 0.0281 - val_acc: 0.9887\n",
      "Epoch 59/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0252 - acc: 0.9895 - val_loss: 0.0219 - val_acc: 0.9909\n",
      "Epoch 60/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0258 - acc: 0.9892 - val_loss: 0.0283 - val_acc: 0.9880\n",
      "Epoch 61/200\n",
      "1999999/1999999 [==============================] - 10s 5us/step - loss: 0.0248 - acc: 0.9898 - val_loss: 0.0262 - val_acc: 0.9889\n",
      "Epoch 62/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0247 - acc: 0.9899 - val_loss: 0.0343 - val_acc: 0.9856\n",
      "Epoch 63/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0276 - acc: 0.9886 - val_loss: 0.0209 - val_acc: 0.9917\n",
      "Epoch 64/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0257 - acc: 0.9892 - val_loss: 0.0273 - val_acc: 0.9891\n",
      "Epoch 65/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0230 - acc: 0.9904 - val_loss: 0.0255 - val_acc: 0.9892\n",
      "Epoch 66/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0261 - acc: 0.9891 - val_loss: 0.0237 - val_acc: 0.9901\n",
      "Epoch 67/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0217 - acc: 0.9909 - val_loss: 0.0241 - val_acc: 0.9895\n",
      "Epoch 68/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0220 - acc: 0.9909 - val_loss: 0.0220 - val_acc: 0.9910\n",
      "Epoch 69/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0248 - acc: 0.9896 - val_loss: 0.0309 - val_acc: 0.9875\n",
      "Epoch 70/200\n",
      "1999999/1999999 [==============================] - 9s 5us/step - loss: 0.0249 - acc: 0.9896 - val_loss: 0.0297 - val_acc: 0.9870\n",
      "Epoch 71/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0265 - acc: 0.9888 - val_loss: 0.0371 - val_acc: 0.9846\n",
      "Epoch 72/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0207 - acc: 0.9914 - val_loss: 0.0218 - val_acc: 0.9908\n",
      "Epoch 73/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0231 - acc: 0.9904 - val_loss: 0.0252 - val_acc: 0.9894\n",
      "Epoch 74/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0227 - acc: 0.9905 - val_loss: 0.0292 - val_acc: 0.9879\n",
      "Epoch 75/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0241 - acc: 0.9900 - val_loss: 0.0260 - val_acc: 0.9893\n",
      "Epoch 76/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0258 - acc: 0.9893 - val_loss: 0.0311 - val_acc: 0.9872\n",
      "Epoch 77/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0221 - acc: 0.9907 - val_loss: 0.0258 - val_acc: 0.9892\n",
      "Epoch 78/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0233 - acc: 0.9902 - val_loss: 0.0210 - val_acc: 0.9913\n",
      "Epoch 79/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0233 - acc: 0.9904 - val_loss: 0.0354 - val_acc: 0.9855\n",
      "Epoch 80/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0243 - acc: 0.9898 - val_loss: 0.0263 - val_acc: 0.9886\n",
      "Epoch 81/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0212 - acc: 0.9912 - val_loss: 0.0295 - val_acc: 0.9875\n",
      "Epoch 82/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0221 - acc: 0.9907 - val_loss: 0.0206 - val_acc: 0.9914\n",
      "Epoch 83/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0235 - acc: 0.9901 - val_loss: 0.0210 - val_acc: 0.9911\n",
      "Epoch 84/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0218 - acc: 0.9908 - val_loss: 0.0278 - val_acc: 0.9882\n",
      "Epoch 85/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0253 - acc: 0.9895 - val_loss: 0.0280 - val_acc: 0.9884\n",
      "Epoch 86/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0226 - acc: 0.9906 - val_loss: 0.0236 - val_acc: 0.9901\n",
      "Epoch 87/200\n",
      "1999999/1999999 [==============================] - 9s 4us/step - loss: 0.0200 - acc: 0.9917 - val_loss: 0.0192 - val_acc: 0.9920\n",
      "Epoch 88/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0200 - acc: 0.9916 - val_loss: 0.0369 - val_acc: 0.9861\n",
      "Epoch 89/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0233 - acc: 0.9902 - val_loss: 0.0246 - val_acc: 0.9898\n",
      "Epoch 90/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0248 - acc: 0.9896 - val_loss: 0.0286 - val_acc: 0.9883\n",
      "Epoch 91/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0224 - acc: 0.9906 - val_loss: 0.0206 - val_acc: 0.9914\n",
      "Epoch 92/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0207 - acc: 0.9914 - val_loss: 0.0255 - val_acc: 0.9894\n",
      "Epoch 93/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0235 - acc: 0.9902 - val_loss: 0.0231 - val_acc: 0.9904\n",
      "Epoch 94/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0210 - acc: 0.9912 - val_loss: 0.0206 - val_acc: 0.9914\n",
      "Epoch 95/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0217 - acc: 0.9910 - val_loss: 0.0334 - val_acc: 0.9865\n",
      "Epoch 96/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0202 - acc: 0.9915 - val_loss: 0.0216 - val_acc: 0.9908\n",
      "Epoch 97/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0229 - acc: 0.9903 - val_loss: 0.0186 - val_acc: 0.9923\n",
      "Epoch 98/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0211 - acc: 0.9911 - val_loss: 0.0209 - val_acc: 0.9910\n",
      "Epoch 99/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0228 - acc: 0.9904 - val_loss: 0.0202 - val_acc: 0.9917\n",
      "Epoch 100/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0213 - acc: 0.9912 - val_loss: 0.0194 - val_acc: 0.9921\n",
      "Epoch 101/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0227 - acc: 0.9905 - val_loss: 0.0229 - val_acc: 0.9902\n",
      "Epoch 102/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0224 - acc: 0.9907 - val_loss: 0.0370 - val_acc: 0.9845\n",
      "Epoch 103/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0207 - acc: 0.9913 - val_loss: 0.0330 - val_acc: 0.9867\n",
      "Epoch 104/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0223 - acc: 0.9907 - val_loss: 0.0251 - val_acc: 0.9891\n",
      "Epoch 105/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0212 - acc: 0.9911 - val_loss: 0.0200 - val_acc: 0.9916\n",
      "Epoch 106/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0225 - acc: 0.9906 - val_loss: 0.0324 - val_acc: 0.9863\n",
      "Epoch 107/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0217 - acc: 0.9910 - val_loss: 0.0175 - val_acc: 0.9928\n",
      "Epoch 108/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0196 - acc: 0.9918 - val_loss: 0.0271 - val_acc: 0.9882\n",
      "Epoch 109/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0207 - acc: 0.9913 - val_loss: 0.0271 - val_acc: 0.9888\n",
      "Epoch 110/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0226 - acc: 0.9906 - val_loss: 0.0224 - val_acc: 0.9903\n",
      "Epoch 111/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0197 - acc: 0.9916 - val_loss: 0.0194 - val_acc: 0.9920\n",
      "Epoch 112/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0215 - acc: 0.9911 - val_loss: 0.0301 - val_acc: 0.9873\n",
      "Epoch 113/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0220 - acc: 0.9908 - val_loss: 0.0328 - val_acc: 0.9874\n",
      "Epoch 114/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0222 - acc: 0.9907 - val_loss: 0.0192 - val_acc: 0.9918\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0194 - acc: 0.9918 - val_loss: 0.0178 - val_acc: 0.9925\n",
      "Epoch 116/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0236 - acc: 0.9903 - val_loss: 0.0315 - val_acc: 0.9866\n",
      "Epoch 117/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0219 - acc: 0.9907 - val_loss: 0.0233 - val_acc: 0.9903\n",
      "Epoch 118/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0252 - acc: 0.9896 - val_loss: 0.0269 - val_acc: 0.9887\n",
      "Epoch 119/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0190 - acc: 0.9920 - val_loss: 0.0302 - val_acc: 0.9879\n",
      "Epoch 120/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0208 - acc: 0.9911 - val_loss: 0.0230 - val_acc: 0.9901\n",
      "Epoch 121/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0193 - acc: 0.9919 - val_loss: 0.0237 - val_acc: 0.9897\n",
      "Epoch 122/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0213 - acc: 0.9910 - val_loss: 0.0229 - val_acc: 0.9901\n",
      "Epoch 123/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0231 - acc: 0.9904 - val_loss: 0.0226 - val_acc: 0.9904\n",
      "Epoch 124/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0220 - acc: 0.9907 - val_loss: 0.0312 - val_acc: 0.9872\n",
      "Epoch 125/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0194 - acc: 0.9920 - val_loss: 0.0273 - val_acc: 0.9886\n",
      "Epoch 126/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0221 - acc: 0.9908 - val_loss: 0.0190 - val_acc: 0.9919\n",
      "Epoch 127/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0203 - acc: 0.9916 - val_loss: 0.0225 - val_acc: 0.9908\n",
      "Epoch 128/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0207 - acc: 0.9914 - val_loss: 0.0244 - val_acc: 0.9896\n",
      "Epoch 129/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0220 - acc: 0.9909 - val_loss: 0.0328 - val_acc: 0.9874\n",
      "Epoch 130/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0199 - acc: 0.9917 - val_loss: 0.0243 - val_acc: 0.9893\n",
      "Epoch 131/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0201 - acc: 0.9915 - val_loss: 0.0243 - val_acc: 0.9896\n",
      "Epoch 132/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0195 - acc: 0.9918 - val_loss: 0.0235 - val_acc: 0.9901\n",
      "Epoch 133/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0193 - acc: 0.9918 - val_loss: 0.0227 - val_acc: 0.9901\n",
      "Epoch 134/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0196 - acc: 0.9917 - val_loss: 0.0219 - val_acc: 0.9908\n",
      "Epoch 135/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0201 - acc: 0.9916 - val_loss: 0.0218 - val_acc: 0.9909\n",
      "Epoch 136/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0236 - acc: 0.9903 - val_loss: 0.0193 - val_acc: 0.9917\n",
      "Epoch 137/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0212 - acc: 0.9911 - val_loss: 0.0217 - val_acc: 0.9910\n",
      "Epoch 138/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0197 - acc: 0.9917 - val_loss: 0.0189 - val_acc: 0.9920\n",
      "Epoch 139/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0213 - acc: 0.9911 - val_loss: 0.0366 - val_acc: 0.9858\n",
      "Epoch 140/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0211 - acc: 0.9912 - val_loss: 0.0246 - val_acc: 0.9898\n",
      "Epoch 141/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0203 - acc: 0.9916 - val_loss: 0.0189 - val_acc: 0.9924\n",
      "Epoch 142/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0188 - acc: 0.9921 - val_loss: 0.0243 - val_acc: 0.9893\n",
      "Epoch 143/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0225 - acc: 0.9906 - val_loss: 0.0228 - val_acc: 0.9901\n",
      "Epoch 144/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0208 - acc: 0.9912 - val_loss: 0.0223 - val_acc: 0.9906\n",
      "Epoch 145/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0196 - acc: 0.9917 - val_loss: 0.0216 - val_acc: 0.9909\n",
      "Epoch 146/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0212 - acc: 0.9910 - val_loss: 0.0292 - val_acc: 0.9877\n",
      "Epoch 147/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0183 - acc: 0.9923 - val_loss: 0.0183 - val_acc: 0.9924\n",
      "Epoch 148/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0188 - acc: 0.9921 - val_loss: 0.0289 - val_acc: 0.9881\n",
      "Epoch 149/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0223 - acc: 0.9907 - val_loss: 0.0232 - val_acc: 0.9901\n",
      "Epoch 150/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0173 - acc: 0.9928 - val_loss: 0.0267 - val_acc: 0.9890\n",
      "Epoch 151/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0217 - acc: 0.9910 - val_loss: 0.0168 - val_acc: 0.9930\n",
      "Epoch 152/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0189 - acc: 0.9920 - val_loss: 0.0270 - val_acc: 0.9884\n",
      "Epoch 153/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0239 - acc: 0.9900 - val_loss: 0.0204 - val_acc: 0.9910\n",
      "Epoch 154/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0185 - acc: 0.9922 - val_loss: 0.0220 - val_acc: 0.9904\n",
      "Epoch 155/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0179 - acc: 0.9924 - val_loss: 0.0287 - val_acc: 0.9880\n",
      "Epoch 156/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0203 - acc: 0.9914 - val_loss: 0.0279 - val_acc: 0.9889\n",
      "Epoch 157/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0216 - acc: 0.9910 - val_loss: 0.0183 - val_acc: 0.9922\n",
      "Epoch 158/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0181 - acc: 0.9923 - val_loss: 0.0187 - val_acc: 0.9919\n",
      "Epoch 159/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0201 - acc: 0.9915 - val_loss: 0.0284 - val_acc: 0.9887\n",
      "Epoch 160/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0195 - acc: 0.9917 - val_loss: 0.0291 - val_acc: 0.9880\n",
      "Epoch 161/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0187 - acc: 0.9920 - val_loss: 0.0284 - val_acc: 0.9886\n",
      "Epoch 162/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0207 - acc: 0.9913 - val_loss: 0.0179 - val_acc: 0.9924\n",
      "Epoch 163/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0184 - acc: 0.9922 - val_loss: 0.0218 - val_acc: 0.9905\n",
      "Epoch 164/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0187 - acc: 0.9921 - val_loss: 0.0242 - val_acc: 0.9899\n",
      "Epoch 165/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0196 - acc: 0.9917 - val_loss: 0.0240 - val_acc: 0.9905\n",
      "Epoch 166/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0203 - acc: 0.9914 - val_loss: 0.0304 - val_acc: 0.9882\n",
      "Epoch 167/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0218 - acc: 0.9908 - val_loss: 0.0225 - val_acc: 0.9903\n",
      "Epoch 168/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0184 - acc: 0.9922 - val_loss: 0.0254 - val_acc: 0.9896\n",
      "Epoch 169/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0201 - acc: 0.9916 - val_loss: 0.0280 - val_acc: 0.9884\n",
      "Epoch 170/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0202 - acc: 0.9916 - val_loss: 0.0182 - val_acc: 0.9921\n",
      "Epoch 171/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0182 - acc: 0.9924 - val_loss: 0.0245 - val_acc: 0.9893\n",
      "Epoch 172/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0178 - acc: 0.9924 - val_loss: 0.0220 - val_acc: 0.9906\n",
      "Epoch 173/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0203 - acc: 0.9914 - val_loss: 0.0219 - val_acc: 0.9906\n",
      "Epoch 174/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0177 - acc: 0.9925 - val_loss: 0.0215 - val_acc: 0.9908\n",
      "Epoch 175/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0204 - acc: 0.9915 - val_loss: 0.0181 - val_acc: 0.9923\n",
      "Epoch 176/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0213 - acc: 0.9909 - val_loss: 0.0181 - val_acc: 0.9925\n",
      "Epoch 177/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0213 - acc: 0.9911 - val_loss: 0.0176 - val_acc: 0.9926\n",
      "Epoch 178/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0177 - acc: 0.9926 - val_loss: 0.0205 - val_acc: 0.9914\n",
      "Epoch 179/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0199 - acc: 0.9916 - val_loss: 0.0243 - val_acc: 0.9899\n",
      "Epoch 180/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0181 - acc: 0.9924 - val_loss: 0.0275 - val_acc: 0.9891\n",
      "Epoch 181/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0184 - acc: 0.9922 - val_loss: 0.0223 - val_acc: 0.9912\n",
      "Epoch 182/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0230 - acc: 0.9903 - val_loss: 0.0312 - val_acc: 0.9879\n",
      "Epoch 183/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0189 - acc: 0.9920 - val_loss: 0.0198 - val_acc: 0.9916\n",
      "Epoch 184/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0196 - acc: 0.9916 - val_loss: 0.0170 - val_acc: 0.9930\n",
      "Epoch 185/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0179 - acc: 0.9925 - val_loss: 0.0191 - val_acc: 0.9918\n",
      "Epoch 186/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0194 - acc: 0.9919 - val_loss: 0.0164 - val_acc: 0.9933\n",
      "Epoch 187/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0201 - acc: 0.9916 - val_loss: 0.0168 - val_acc: 0.9929\n",
      "Epoch 188/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0163 - acc: 0.9931 - val_loss: 0.0229 - val_acc: 0.9904\n",
      "Epoch 189/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0220 - acc: 0.9909 - val_loss: 0.0207 - val_acc: 0.9913\n",
      "Epoch 190/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0209 - acc: 0.9912 - val_loss: 0.0310 - val_acc: 0.9882\n",
      "Epoch 191/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0193 - acc: 0.9918 - val_loss: 0.0288 - val_acc: 0.9891\n",
      "Epoch 192/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0181 - acc: 0.9924 - val_loss: 0.0308 - val_acc: 0.9876\n",
      "Epoch 193/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0192 - acc: 0.9918 - val_loss: 0.0229 - val_acc: 0.9900\n",
      "Epoch 194/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0233 - acc: 0.9904 - val_loss: 0.0217 - val_acc: 0.9908\n",
      "Epoch 195/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0194 - acc: 0.9919 - val_loss: 0.0242 - val_acc: 0.9895\n",
      "Epoch 196/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0195 - acc: 0.9917 - val_loss: 0.0270 - val_acc: 0.9889\n",
      "Epoch 197/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0173 - acc: 0.9927 - val_loss: 0.0194 - val_acc: 0.9919\n",
      "Epoch 198/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0193 - acc: 0.9918 - val_loss: 0.0191 - val_acc: 0.9920\n",
      "Epoch 199/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0180 - acc: 0.9924 - val_loss: 0.0228 - val_acc: 0.9905\n",
      "Epoch 200/200\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.0205 - acc: 0.9915 - val_loss: 0.0171 - val_acc: 0.9929\n"
     ]
    }
   ],
   "source": [
    "model_re.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "histor = model_re.fit(puf,res, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(test,res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,545\n",
      "Trainable params: 12,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import RandomNormal\n",
    "model_re = Sequential()\n",
    "model_re.add(Dense(64, activation='relu', input_shape=(input_dim,),kernel_initializer='he_uniform'))\n",
    "model_re.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_re.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "#model_re.add(Dense(64, activation='relu'))\n",
    "model_re.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "model_re.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.994, Test: 0.993\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = model_re.evaluate(puf,res, verbose=0)\n",
    "_, test_acc = model_re.evaluate(test,res1, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Iteration 1, loss = 0.14050597\n",
      "Validation score: 1.000000\n",
      "Iteration 2, loss = 0.00026069\n",
      "Validation score: 1.000000\n",
      "Iteration 3, loss = 0.00010576\n",
      "Validation score: 1.000000\n",
      "Iteration 4, loss = 0.00005642\n",
      "Validation score: 1.000000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Training set score: 1.000000\n",
      "val set score: 1.000000\n",
      "Test set score: 0.500194\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,64,64), max_iter=200, alpha=1e-5,batch_size=10000,early_stopping=True,activation='relu',solver='adam', verbose=10, tol=1e-4, random_state=1,learning_rate='adaptive',\n",
    "                    learning_rate_init=.001)\n",
    "\n",
    "mlp.fit(X_train1,y_train1)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train1,y_train1))\n",
    "print(\"val set score: %f\" % mlp.score(X_test1,y_test1))\n",
    "print(\"Test set score: %f\" % mlp.score(test,res1))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n",
    "# https://stackoverflow.com/a/14434334\n",
    "# this function is used to update the plots for each epoch and error\n",
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.018871261181278194\n",
      "Test accuracy: 0.9926749816874543\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = model_re.evaluate(test,res1, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('binary Crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,nb_epoch+1))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
    "# val_loss : validation loss\n",
    "# val_acc : validation accuracy\n",
    "\n",
    "# loss : training loss\n",
    "# acc : train accuracy\n",
    "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
    "\n",
    "vy = history.history['val_loss']\n",
    "ty = history.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,545\n",
      "Trainable params: 12,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_re = Sequential()\n",
    "model_re.add(Dense(64, activation='relu', input_shape=(64,),kernel_initializer='glorot_uniform'))\n",
    "model_re.add(Dense(64, activation='relu',kernel_initializer='glorot_uniform'))\n",
    "model_re.add(Dense(64, activation='relu',kernel_initializer='glorot_uniform'))\n",
    "#model_re.add(Dense(64, activation='relu'))\n",
    "model_re.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_re.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1999999 samples, validate on 399999 samples\n",
      "Epoch 1/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6933 - acc: 0.5018 - val_loss: 0.6933 - val_acc: 0.5010\n",
      "Epoch 2/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6931 - acc: 0.5039 - val_loss: 0.6933 - val_acc: 0.5007\n",
      "Epoch 3/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6930 - acc: 0.5060 - val_loss: 0.6934 - val_acc: 0.5011\n",
      "Epoch 4/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6929 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.5016\n",
      "Epoch 5/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6928 - acc: 0.5099 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 6/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6926 - acc: 0.5114 - val_loss: 0.6934 - val_acc: 0.5014\n",
      "Epoch 7/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6925 - acc: 0.5133 - val_loss: 0.6935 - val_acc: 0.5022\n",
      "Epoch 8/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6923 - acc: 0.5146 - val_loss: 0.6935 - val_acc: 0.5024\n",
      "Epoch 9/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6921 - acc: 0.5162 - val_loss: 0.6937 - val_acc: 0.5016\n",
      "Epoch 10/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6919 - acc: 0.5172 - val_loss: 0.6936 - val_acc: 0.5028\n",
      "Epoch 11/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6916 - acc: 0.5185 - val_loss: 0.6936 - val_acc: 0.5034\n",
      "Epoch 12/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6913 - acc: 0.5195 - val_loss: 0.6936 - val_acc: 0.5029\n",
      "Epoch 13/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6910 - acc: 0.5210 - val_loss: 0.6936 - val_acc: 0.5023\n",
      "Epoch 14/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6906 - acc: 0.5222 - val_loss: 0.6935 - val_acc: 0.5036\n",
      "Epoch 15/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6902 - acc: 0.5235 - val_loss: 0.6932 - val_acc: 0.5039\n",
      "Epoch 16/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6898 - acc: 0.5248 - val_loss: 0.6932 - val_acc: 0.5049\n",
      "Epoch 17/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6895 - acc: 0.5252 - val_loss: 0.6929 - val_acc: 0.5053\n",
      "Epoch 18/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6891 - acc: 0.5268 - val_loss: 0.6930 - val_acc: 0.5051\n",
      "Epoch 19/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6887 - acc: 0.5278 - val_loss: 0.6925 - val_acc: 0.5080\n",
      "Epoch 20/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6881 - acc: 0.5292 - val_loss: 0.6918 - val_acc: 0.5090\n",
      "Epoch 21/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6873 - acc: 0.5304 - val_loss: 0.6908 - val_acc: 0.5119\n",
      "Epoch 22/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6859 - acc: 0.5325 - val_loss: 0.6885 - val_acc: 0.5169\n",
      "Epoch 23/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6827 - acc: 0.5365 - val_loss: 0.6829 - val_acc: 0.5240\n",
      "Epoch 24/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.6732 - acc: 0.5467 - val_loss: 0.6612 - val_acc: 0.5616\n",
      "Epoch 25/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.6416 - acc: 0.5870 - val_loss: 0.6230 - val_acc: 0.6082\n",
      "Epoch 26/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.5940 - acc: 0.6466 - val_loss: 0.5591 - val_acc: 0.6886\n",
      "Epoch 27/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.5189 - acc: 0.7238 - val_loss: 0.4787 - val_acc: 0.7561\n",
      "Epoch 28/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.4360 - acc: 0.7863 - val_loss: 0.3959 - val_acc: 0.8111\n",
      "Epoch 29/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.3565 - acc: 0.8352 - val_loss: 0.3216 - val_acc: 0.8546\n",
      "Epoch 30/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.2930 - acc: 0.8707 - val_loss: 0.2713 - val_acc: 0.8810\n",
      "Epoch 31/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.2487 - acc: 0.8939 - val_loss: 0.2321 - val_acc: 0.9016\n",
      "Epoch 32/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.2167 - acc: 0.9095 - val_loss: 0.2050 - val_acc: 0.9146\n",
      "Epoch 33/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1922 - acc: 0.9209 - val_loss: 0.1839 - val_acc: 0.9237\n",
      "Epoch 34/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1728 - acc: 0.9294 - val_loss: 0.1651 - val_acc: 0.9336\n",
      "Epoch 35/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1562 - acc: 0.9369 - val_loss: 0.1508 - val_acc: 0.9396\n",
      "Epoch 36/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1431 - acc: 0.9427 - val_loss: 0.1412 - val_acc: 0.9433\n",
      "Epoch 37/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1329 - acc: 0.9469 - val_loss: 0.1306 - val_acc: 0.9471\n",
      "Epoch 38/140\n",
      "1999999/1999999 [==============================] - 8s 4us/step - loss: 0.1243 - acc: 0.9503 - val_loss: 0.1204 - val_acc: 0.9525\n",
      "Epoch 39/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.1168 - acc: 0.9538 - val_loss: 0.1141 - val_acc: 0.9546\n",
      "Epoch 40/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.1098 - acc: 0.9567 - val_loss: 0.1080 - val_acc: 0.9576\n",
      "Epoch 41/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.1042 - acc: 0.9591 - val_loss: 0.1037 - val_acc: 0.9584\n",
      "Epoch 42/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0991 - acc: 0.9610 - val_loss: 0.0975 - val_acc: 0.9616\n",
      "Epoch 43/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0943 - acc: 0.9632 - val_loss: 0.0926 - val_acc: 0.9640\n",
      "Epoch 44/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0895 - acc: 0.9653 - val_loss: 0.0880 - val_acc: 0.9658\n",
      "Epoch 45/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0855 - acc: 0.9668 - val_loss: 0.0846 - val_acc: 0.9674\n",
      "Epoch 46/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0817 - acc: 0.9684 - val_loss: 0.0820 - val_acc: 0.9676\n",
      "Epoch 47/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0784 - acc: 0.9697 - val_loss: 0.0773 - val_acc: 0.9698\n",
      "Epoch 48/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0749 - acc: 0.9711 - val_loss: 0.0744 - val_acc: 0.9712\n",
      "Epoch 49/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0719 - acc: 0.9723 - val_loss: 0.0704 - val_acc: 0.9732\n",
      "Epoch 50/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0689 - acc: 0.9737 - val_loss: 0.0698 - val_acc: 0.9727\n",
      "Epoch 51/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0667 - acc: 0.9744 - val_loss: 0.0660 - val_acc: 0.9746\n",
      "Epoch 52/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0635 - acc: 0.9758 - val_loss: 0.0647 - val_acc: 0.9754\n",
      "Epoch 53/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0612 - acc: 0.9767 - val_loss: 0.0606 - val_acc: 0.9772\n",
      "Epoch 54/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0590 - acc: 0.9775 - val_loss: 0.0593 - val_acc: 0.9778\n",
      "Epoch 55/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0569 - acc: 0.9782 - val_loss: 0.0577 - val_acc: 0.9778\n",
      "Epoch 56/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0550 - acc: 0.9790 - val_loss: 0.0572 - val_acc: 0.9772\n",
      "Epoch 57/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0536 - acc: 0.9796 - val_loss: 0.0548 - val_acc: 0.9787\n",
      "Epoch 58/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0519 - acc: 0.9802 - val_loss: 0.0512 - val_acc: 0.9813\n",
      "Epoch 59/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0501 - acc: 0.9811 - val_loss: 0.0510 - val_acc: 0.9800\n",
      "Epoch 60/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0484 - acc: 0.9817 - val_loss: 0.0529 - val_acc: 0.9787\n",
      "Epoch 61/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0483 - acc: 0.9813 - val_loss: 0.0498 - val_acc: 0.9805\n",
      "Epoch 62/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0458 - acc: 0.9826 - val_loss: 0.0460 - val_acc: 0.9826\n",
      "Epoch 63/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0441 - acc: 0.9834 - val_loss: 0.0471 - val_acc: 0.9811\n",
      "Epoch 64/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 65/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0416 - acc: 0.9843 - val_loss: 0.0433 - val_acc: 0.9838\n",
      "Epoch 66/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0407 - val_acc: 0.9846\n",
      "Epoch 67/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0400 - acc: 0.9848 - val_loss: 0.0405 - val_acc: 0.9850\n",
      "Epoch 68/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0381 - acc: 0.9857 - val_loss: 0.0388 - val_acc: 0.9854\n",
      "Epoch 69/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0378 - acc: 0.9856 - val_loss: 0.0388 - val_acc: 0.9848\n",
      "Epoch 70/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0362 - acc: 0.9864 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "Epoch 71/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0353 - acc: 0.9868 - val_loss: 0.0380 - val_acc: 0.9856\n",
      "Epoch 72/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0351 - acc: 0.9867 - val_loss: 0.0351 - val_acc: 0.9865\n",
      "Epoch 73/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0341 - acc: 0.9871 - val_loss: 0.0338 - val_acc: 0.9876\n",
      "Epoch 74/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0331 - acc: 0.9876 - val_loss: 0.0344 - val_acc: 0.9868\n",
      "Epoch 75/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0329 - acc: 0.9874 - val_loss: 0.0334 - val_acc: 0.9875\n",
      "Epoch 76/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0320 - acc: 0.9879 - val_loss: 0.0342 - val_acc: 0.9869\n",
      "Epoch 77/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0310 - acc: 0.9883 - val_loss: 0.0316 - val_acc: 0.9883\n",
      "Epoch 78/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0307 - acc: 0.9883 - val_loss: 0.0317 - val_acc: 0.9881\n",
      "Epoch 79/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0305 - acc: 0.9883 - val_loss: 0.0319 - val_acc: 0.9875\n",
      "Epoch 80/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0293 - acc: 0.9889 - val_loss: 0.0317 - val_acc: 0.9879\n",
      "Epoch 81/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0296 - acc: 0.9886 - val_loss: 0.0320 - val_acc: 0.9873\n",
      "Epoch 82/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0291 - acc: 0.9889 - val_loss: 0.0303 - val_acc: 0.9884\n",
      "Epoch 83/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0279 - acc: 0.9894 - val_loss: 0.0285 - val_acc: 0.9896\n",
      "Epoch 84/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0280 - acc: 0.9893 - val_loss: 0.0297 - val_acc: 0.9887\n",
      "Epoch 85/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0269 - acc: 0.9898 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 86/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0275 - acc: 0.9892 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 87/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0265 - acc: 0.9899 - val_loss: 0.0267 - val_acc: 0.9902\n",
      "Epoch 88/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0258 - acc: 0.9902 - val_loss: 0.0262 - val_acc: 0.9904\n",
      "Epoch 89/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0261 - acc: 0.9900 - val_loss: 0.0266 - val_acc: 0.9900\n",
      "Epoch 90/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0253 - acc: 0.9902 - val_loss: 0.0260 - val_acc: 0.9903\n",
      "Epoch 91/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0251 - acc: 0.9903 - val_loss: 0.0277 - val_acc: 0.9890\n",
      "Epoch 92/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0244 - acc: 0.9907 - val_loss: 0.0260 - val_acc: 0.9905\n",
      "Epoch 93/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0244 - acc: 0.9906 - val_loss: 0.0285 - val_acc: 0.9890\n",
      "Epoch 94/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0247 - acc: 0.9902 - val_loss: 0.0304 - val_acc: 0.9868\n",
      "Epoch 95/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0241 - acc: 0.9906 - val_loss: 0.0245 - val_acc: 0.9909\n",
      "Epoch 96/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0237 - acc: 0.9908 - val_loss: 0.0289 - val_acc: 0.9884\n",
      "Epoch 97/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0237 - acc: 0.9907 - val_loss: 0.0248 - val_acc: 0.9907\n",
      "Epoch 98/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0230 - acc: 0.9910 - val_loss: 0.0233 - val_acc: 0.9916\n",
      "Epoch 99/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0226 - acc: 0.9912 - val_loss: 0.0234 - val_acc: 0.9914\n",
      "Epoch 100/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0229 - acc: 0.9910 - val_loss: 0.0269 - val_acc: 0.9898\n",
      "Epoch 101/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0221 - acc: 0.9913 - val_loss: 0.0250 - val_acc: 0.9904\n",
      "Epoch 102/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0221 - acc: 0.9914 - val_loss: 0.0243 - val_acc: 0.9905\n",
      "Epoch 103/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0213 - acc: 0.9918 - val_loss: 0.0265 - val_acc: 0.9898\n",
      "Epoch 104/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0224 - acc: 0.9912 - val_loss: 0.0224 - val_acc: 0.9916\n",
      "Epoch 105/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0212 - acc: 0.9918 - val_loss: 0.0238 - val_acc: 0.9909\n",
      "Epoch 106/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0224 - acc: 0.9911 - val_loss: 0.0250 - val_acc: 0.9902\n",
      "Epoch 107/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0213 - acc: 0.9917 - val_loss: 0.0225 - val_acc: 0.9914\n",
      "Epoch 108/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0206 - acc: 0.9919 - val_loss: 0.0225 - val_acc: 0.9912\n",
      "Epoch 109/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0205 - acc: 0.9920 - val_loss: 0.0228 - val_acc: 0.9913\n",
      "Epoch 110/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0209 - acc: 0.9918 - val_loss: 0.0219 - val_acc: 0.9915\n",
      "Epoch 111/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0203 - acc: 0.9919 - val_loss: 0.0221 - val_acc: 0.9913\n",
      "Epoch 112/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0205 - acc: 0.9919 - val_loss: 0.0277 - val_acc: 0.9886\n",
      "Epoch 113/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0211 - acc: 0.9916 - val_loss: 0.0231 - val_acc: 0.9912\n",
      "Epoch 114/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0205 - acc: 0.9919 - val_loss: 0.0216 - val_acc: 0.9916\n",
      "Epoch 115/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0196 - acc: 0.9923 - val_loss: 0.0212 - val_acc: 0.9916\n",
      "Epoch 116/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0205 - acc: 0.9918 - val_loss: 0.0243 - val_acc: 0.9904\n",
      "Epoch 117/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0194 - acc: 0.9924 - val_loss: 0.0221 - val_acc: 0.9914\n",
      "Epoch 118/140\n",
      "1999999/1999999 [==============================] - 7s 3us/step - loss: 0.0200 - acc: 0.9920 - val_loss: 0.0202 - val_acc: 0.9924\n",
      "Epoch 119/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0197 - acc: 0.9920 - val_loss: 0.0219 - val_acc: 0.9915\n",
      "Epoch 120/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0200 - acc: 0.9922 - val_loss: 0.0213 - val_acc: 0.9916\n",
      "Epoch 121/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0190 - acc: 0.9924 - val_loss: 0.0228 - val_acc: 0.9912\n",
      "Epoch 122/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0194 - acc: 0.9921 - val_loss: 0.0217 - val_acc: 0.9916\n",
      "Epoch 123/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0185 - acc: 0.9927 - val_loss: 0.0233 - val_acc: 0.9908\n",
      "Epoch 124/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0184 - acc: 0.9927 - val_loss: 0.0207 - val_acc: 0.9919\n",
      "Epoch 125/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0186 - acc: 0.9925 - val_loss: 0.0212 - val_acc: 0.9918\n",
      "Epoch 126/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0189 - acc: 0.9924 - val_loss: 0.0192 - val_acc: 0.9928\n",
      "Epoch 127/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0182 - acc: 0.9928 - val_loss: 0.0195 - val_acc: 0.9927\n",
      "Epoch 128/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0181 - acc: 0.9927 - val_loss: 0.0234 - val_acc: 0.9909\n",
      "Epoch 129/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0187 - acc: 0.9924 - val_loss: 0.0183 - val_acc: 0.9931\n",
      "Epoch 130/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0180 - acc: 0.9929 - val_loss: 0.0219 - val_acc: 0.9915\n",
      "Epoch 131/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0189 - acc: 0.9924 - val_loss: 0.0201 - val_acc: 0.9921\n",
      "Epoch 132/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0181 - acc: 0.9927 - val_loss: 0.0218 - val_acc: 0.9913\n",
      "Epoch 133/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0179 - acc: 0.9928 - val_loss: 0.0238 - val_acc: 0.9904\n",
      "Epoch 134/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0189 - acc: 0.9923 - val_loss: 0.0295 - val_acc: 0.9877\n",
      "Epoch 135/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0184 - acc: 0.9926 - val_loss: 0.0197 - val_acc: 0.9920\n",
      "Epoch 136/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0179 - acc: 0.9928 - val_loss: 0.0196 - val_acc: 0.9926\n",
      "Epoch 137/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0177 - acc: 0.9930 - val_loss: 0.0195 - val_acc: 0.9923\n",
      "Epoch 138/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0175 - acc: 0.9929 - val_loss: 0.0196 - val_acc: 0.9923\n",
      "Epoch 139/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0171 - acc: 0.9931 - val_loss: 0.0220 - val_acc: 0.9917\n",
      "Epoch 140/140\n",
      "1999999/1999999 [==============================] - 7s 4us/step - loss: 0.0175 - acc: 0.9929 - val_loss: 0.0189 - val_acc: 0.9927\n"
     ]
    }
   ],
   "source": [
    "model_re.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "histo = model_re.fit(puf,res,batch_size=10000, epochs=140, verbose=1, validation_data=(test,res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model               |   Accuracy |\n",
      "|---------------------+------------|\n",
      "| Logistic regression |      50.14 |\n",
      "| MLPClassifier       |      98.64 |\n",
      "| Sequential model    |      99.54 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([['Logistic regression',50.14],['MLPClassifier',98.64], ['Sequential model', 99.54]], headers=['Model', 'Accuracy'], tablefmt='orgtbl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
